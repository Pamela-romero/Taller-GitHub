
# üöó Detecci√≥n de Fraude en Seguros de Autom√≥viles

Este proyecto tiene como objetivo identificar posibles fraudes en reclamos de seguros de autom√≥viles utilizando t√©cnicas de Machine Learning. Se emplean varios modelos, se tratan los datos desbalanceados, y se optimizan hiperpar√°metros para mejorar el desempe√±o.

---

## üìå 1. Selecci√≥n de Caracter√≠sticas

```python
features = [ ... ]
```

Se define una lista de caracter√≠sticas que ser√°n utilizadas como variables predictoras. Estas incluyen variables ordinales, categ√≥ricas codificadas (one-hot) y binarias.

---

## üß† 2. Modelos de Clasificaci√≥n

```python
model1 = DecisionTreeClassifier(...)
model2 = KNeighborsClassifier()
model3 = XGBClassifier(...)
model4 = SVC(...)
```

Se crean cuatro modelos de clasificaci√≥n distintos para luego integrarlos en un ensamblado con VotingClassifier:
- √Årbol de decisi√≥n
- K-Vecinos m√°s cercanos
- XGBoost (modelo base robusto)
- SVM (con probabilidades activadas)

---

## ü§ñ 3. Ensamblado con VotingClassifier

```python
voting_clf = VotingClassifier(estimators=[...], voting='soft')
```

Se construye un modelo de ensamblado que utiliza votaci√≥n **soft**, lo que significa que se consideran las probabilidades de predicci√≥n de cada modelo para hacer una predicci√≥n final. Esto tiende a mejorar el rendimiento si los modelos base son diversos.

---

## üìä 4. Evaluaci√≥n Inicial de Modelos

```python
classification_report(...)
```

Se eval√∫an individualmente los modelos dentro del ensamble utilizando el conjunto de entrenamiento. Se imprime la precisi√≥n, recall y F1-score para entender c√≥mo se comportan con datos desbalanceados (clase minoritaria: fraude).

---

## ‚öñÔ∏è 5. Manejo del Desbalanceo

```python
smote = SMOTE(...)
X_resampled, y_resampled = smote.fit_resample(...)
```

Se utiliza SMOTE para sobresamplear la clase minoritaria (fraude). Esto busca mejorar la capacidad del modelo para aprender patrones de esta clase y reducir falsos negativos.

---

## ‚öôÔ∏è 6. Ajuste de Hiperpar√°metros con GridSearchCV

```python
grid_search = GridSearchCV(...)
```

Se entrena un modelo **XGBoost** ajustando autom√°ticamente sus hiperpar√°metros con `GridSearchCV`, optimizando para el F1-score macro. Se calcula `scale_pos_weight` para ajustar el desequilibrio de clases.

---

## üß™ 7. Evaluaci√≥n del Modelo √ìptimo

```python
classification_report(...)
```

Con los mejores par√°metros, se eval√∫a el modelo sobre el conjunto de validaci√≥n. Se ajusta el umbral de clasificaci√≥n (`0.35`) para mejorar el recall de la clase minoritaria.

---

## üìà 8. Importancia de Caracter√≠sticas

```python
importances = best_model.feature_importances_
```

Se identifican las variables m√°s influyentes en las predicciones del modelo, lo cual ayuda a interpretar c√≥mo toma decisiones el modelo y a posibles mejoras del dataset.

---

## üìâ 9. Matriz de Confusi√≥n

```python
confusion_matrix(...)
ConfusionMatrixDisplay(...).plot()
```

Se visualiza la matriz de confusi√≥n para evaluar el rendimiento en t√©rminos de verdaderos positivos/negativos y falsos positivos/negativos. √ötil para ver errores de clasificaci√≥n en detalle.

---

## üöÄ Requisitos

- Python 3.8+
- scikit-learn
- imbalanced-learn
- xgboost
- pandas
- matplotlib

---

## üë©‚Äçüíª Autor

**Pamela Romero**  
Especialista en Costos | Apasionada por el an√°lisis de datos y el aprendizaje autom√°tico.

---
